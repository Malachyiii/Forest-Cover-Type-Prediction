{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w207 Final Project - Forest Cover Type Prediction\n",
    "\n",
    "Team:\n",
    "   - Diana Chacon\n",
    "   - Jyoti Kumari\n",
    "   - Malachy Moran\n",
    "\n",
    "Date: Aug 01, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have applied most of the classifiers and algorithms for supervised learning, that we studied during w207 MIDS course, starting from week 1 to week 9. These are listed below in the sequence per week:\n",
    "\n",
    "- k Nearest Neighbors (week 2)\n",
    "- Naive Bayes (Week 3)\n",
    "- Decision trees, including Random Forests, Adaboost and Gradient Boosting (week 4)\n",
    "- Logistic regression (week 5)\n",
    "- Stochastic Gradient Descent (week 6)\n",
    "- Support Vector machine (week 8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *The accuracy of the best model will be reported on the dev data, since test data does not have labels. The scope of this project does not include outputting results for the kaggle \"test\" data set, since the kaggle competion is no longer open.*\n",
    "\n",
    "- *There will be no loading or processing done on test data in this whole exercise.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T22:21:45.037432Z",
     "iopub.status.busy": "2021-07-20T22:21:45.036162Z",
     "iopub.status.idle": "2021-07-20T22:21:52.166540Z",
     "shell.execute_reply": "2021-07-20T22:21:52.165720Z",
     "shell.execute_reply.started": "2021-07-20T21:10:40.967588Z"
    },
    "papermill": {
     "duration": 7.177443,
     "end_time": "2021-07-20T22:21:52.166728",
     "exception": false,
     "start_time": "2021-07-20T22:21:44.989285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General libraries.\n",
    "\n",
    "#If your seaborn plots do not run, uncomment the following, run it and restart your kernel\n",
    "#!pip install -U seaborn\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import ensemble\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn libraries for feature preprocessing.\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# SK-learn libraries for dimensionality reduction.\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Data analysis and plotting \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "\n",
    "np.random.seed(0)\n",
    "print (\"OK\")\n",
    "#import tensorflow as tf\n",
    "#print(\"Tensorflow version\", tf.__version__)\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.04212,
     "end_time": "2021-07-20T22:21:52.502219",
     "exception": false,
     "start_time": "2021-07-20T22:21:52.460099",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Part 1: Data Loading, Processing and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Labels**\n",
    "\n",
    "The study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. Each observation is a 30m x 30m patch. You are asked to predict an integer classification for the forest cover type. The seven types are:\n",
    "\n",
    "1 - Spruce/Fir\n",
    "2 - Lodgepole Pine\n",
    "3 - Ponderosa Pine\n",
    "4 - Cottonwood/Willow\n",
    "5 - Aspen\n",
    "6 - Douglas-fir\n",
    "7 - Krummholz\n",
    "\n",
    "The training set (15120 observations) contains both features and the Cover_Type. The test set contains only the features. You must predict the Cover_Type for every row in the test set (565892 observations).\n",
    "\n",
    "**Data Fields**\n",
    "\n",
    "- Elevation - Elevation in meters\n",
    "- Aspect - Aspect in degrees azimuth\n",
    "- Slope - Slope in degrees\n",
    "- Horizontal_Distance_To_Hydrology - Horz Dist to nearest surface water features\n",
    "- Vertical_Distance_To_Hydrology - Vert Dist to nearest surface water features\n",
    "- Horizontal_Distance_To_Roadways - Horz Dist to nearest roadway\n",
    "- Hillshade_9am (0 to 255 index) - Hillshade index at 9am, summer solstice\n",
    "- Hillshade_Noon (0 to 255 index) - Hillshade index at noon, summer solstice\n",
    "- Hillshade_3pm (0 to 255 index) - Hillshade index at 3pm, summer solstice\n",
    "- Horizontal_Distance_To_Fire_Points - Horz Dist to nearest wildfire ignition points\n",
    "- Wilderness_Area (4 binary columns, 0 = absence or 1 = presence) - Wilderness area designation\n",
    "- Soil_Type (40 binary columns, 0 = absence or 1 = presence) - Soil Type designation\n",
    "- Cover_Type (7 types, integers 1 to 7) - Forest Cover Type designation\n",
    "\n",
    "**The wilderness areas are:**\n",
    "\n",
    "- 1 - Rawah Wilderness Area\n",
    "- 2 - Neota Wilderness Area\n",
    "- 3 - Comanche Peak Wilderness Area\n",
    "- 4 - Cache la Poudre Wilderness Area\n",
    "\n",
    "**The soil types are:**\n",
    "\n",
    "- 1 Cathedral family - Rock outcrop complex, extremely stony.\n",
    "- 2 Vanet - Ratake families complex, very stony.\n",
    "- 3 Haploborolis - Rock outcrop complex, rubbly.\n",
    "- 4 Ratake family - Rock outcrop complex, rubbly.\n",
    "- 5 Vanet family - Rock outcrop complex complex, rubbly.\n",
    "- 6 Vanet - Wetmore families - Rock outcrop complex, stony.\n",
    "- 7 Gothic family.\n",
    "- 8 Supervisor - Limber families complex.\n",
    "- 9 Troutville family, very stony.\n",
    "- 10 Bullwark - Catamount families - Rock outcrop complex, rubbly.\n",
    "- 11 Bullwark - Catamount families - Rock land complex, rubbly.\n",
    "- 12 Legault family - Rock land complex, stony.\n",
    "- 13 Catamount family - Rock land - Bullwark family complex, rubbly.\n",
    "- 14 Pachic Argiborolis - Aquolis complex.\n",
    "- 15 unspecified in the USFS Soil and ELU Survey.\n",
    "- 16 Cryaquolis - Cryoborolis complex.\n",
    "- 17 Gateview family - Cryaquolis complex.\n",
    "- 18 Rogert family, very stony.\n",
    "- 19 Typic Cryaquolis - Borohemists complex.\n",
    "- 20 Typic Cryaquepts - Typic Cryaquolls complex.\n",
    "- 21 Typic Cryaquolls - Leighcan family, till substratum complex.\n",
    "- 22 Leighcan family, till substratum, extremely bouldery.\n",
    "- 23 Leighcan family, till substratum - Typic Cryaquolls complex.\n",
    "- 24 Leighcan family, extremely stony.\n",
    "- 25 Leighcan family, warm, extremely stony.\n",
    "- 26 Granile - Catamount families complex, very stony.\n",
    "- 27 Leighcan family, warm - Rock outcrop complex, extremely stony.\n",
    "- 28 Leighcan family - Rock outcrop complex, extremely stony.\n",
    "- 29 Como - Legault families complex, extremely stony.\n",
    "- 30 Como family - Rock land - Legault family complex, extremely stony.\n",
    "- 31 Leighcan - Catamount families complex, extremely stony.\n",
    "- 32 Catamount family - Rock outcrop - Leighcan family complex, extremely stony.\n",
    "- 33 Leighcan - Catamount families - Rock outcrop complex, extremely stony.\n",
    "- 34 Cryorthents - Rock land complex, extremely stony.\n",
    "- 35 Cryumbrepts - Rock outcrop - Cryaquepts complex.\n",
    "- 36 Bross family - Rock land - Cryumbrepts complex, extremely stony.\n",
    "- 37 Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.\n",
    "- 38 Leighcan - Moran families - Cryaquolls complex, extremely stony.\n",
    "- 39 Moran family - Cryorthents - Leighcan family complex, extremely stony.\n",
    "- 40 Moran family - Cryorthents - Rock land complex, extremely stony."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Data Loading\n",
    "\n",
    "**Let's load just the training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading train data\n",
    "train=pd.read_csv('train.csv')\n",
    "\n",
    "train_label = train.Cover_Type #setting the last column `Cover_type` as label for train dataset.\n",
    "\n",
    "# We also discard the 1st variable(ID), which does not provide any information about the forest cover type.\n",
    "train_data = train.drop(['Id', 'Cover_Type'], axis = 1)\n",
    "\n",
    "print(train.shape)\n",
    "print(train_data.shape)\n",
    "print(train_label.shape)\n",
    "\n",
    "#loading test data\n",
    "#test = pd.read_csv('test.csv')\n",
    "#test_data = test.drop(['Id'], axis = 1) # Dropping first column `Id`. There are no labels in the test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "The original train dataset contains 15120 observations with 56 features.(containing both `Id` and the `Cover_Type`)\n",
    "\n",
    "**Explaining the features:**\n",
    "\n",
    "- All features are either continous or binary. There are no text fields.\n",
    "- Soil_Type fields are binary.\n",
    "- Wilderness Area fields are binary.\n",
    "- Without considering the first column, `ID`, The first 10 features of each observation (`Elevation` to `Horizontal_Distance_To_Fire_Points`) are continuous, with different ranges. All 10 features are numeric variables.\n",
    "- 4 of the remaining 44 binary features correspond to `Wilderness_Area` (i.e., there are 4 possible types), so any observation will have one 1 and three 0's in those columns. \n",
    "- The last 40 features correspond to the `Soil_Type` (i.e., there are 40 possible types), so any observation will have one 1 and thirty-nine 0's in those columns.\n",
    "\n",
    "\n",
    "**Training datset:**\n",
    "\n",
    "There are a total of 15120 observations in the training set, that contains 55 features and the `Cover_Type`.\n",
    "\n",
    "As a part of data processing,\n",
    "\n",
    "  - *The first column has been dropped, since it is not really a feature but more of an observation ID*\n",
    " \n",
    "  - *The last column is the train_label*\n",
    "\n",
    "This finally gives us a train dataset with **15120 observations** and **54 features**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split training data into training and validation data (dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To evaluate our performance, we'll split the training set in 2 subsets: training data (67%) and development or validation data (33%).*\n",
    "\n",
    "*Test data must not be used to validate the models, because it introduces bias. Test data must be looked at just once in the whole model building process. Looking at test data multiple times introduces bias and we end up learning the error rate on test data beforehand and try to tweak the training parameters, which is not recommended.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data, but make sure that the features and accompanying labels stay in sync.\n",
    "\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(np.arange(train_data.shape[0]))\n",
    "#train_data = train_data.iloc[shuffle,:]\n",
    "train_data, train_label = train_data.iloc[shuffle], train_label.iloc[shuffle]\n",
    "\n",
    "# Split into train (67%) and dev (33%)\n",
    "\n",
    "train_data, dev_data, train_label, dev_label = train_test_split(train_data, train_label, train_size=0.90)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(train_label.shape)\n",
    "print(dev_data.shape)\n",
    "print(dev_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Describe the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T22:21:56.328557Z",
     "iopub.status.busy": "2021-07-20T22:21:56.327559Z",
     "iopub.status.idle": "2021-07-20T22:21:56.404418Z",
     "shell.execute_reply": "2021-07-20T22:21:56.403815Z",
     "shell.execute_reply.started": "2021-07-20T21:05:33.69494Z"
    },
    "papermill": {
     "duration": 0.12497,
     "end_time": "2021-07-20T22:21:56.404559",
     "exception": false,
     "start_time": "2021-07-20T22:21:56.279589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_data=pd.read_csv('train.csv')\n",
    "\n",
    "print(all_data.head())\n",
    "\n",
    "all_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Look for unique values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T22:21:56.496874Z",
     "iopub.status.busy": "2021-07-20T22:21:56.495785Z",
     "iopub.status.idle": "2021-07-20T22:21:56.529569Z",
     "shell.execute_reply": "2021-07-20T22:21:56.529037Z",
     "shell.execute_reply.started": "2021-07-20T21:05:33.802816Z"
    },
    "papermill": {
     "duration": 0.08133,
     "end_time": "2021-07-20T22:21:56.529725",
     "exception": false,
     "start_time": "2021-07-20T22:21:56.448395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Looking for the total number of unique values by column\n",
    "all_data.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Look for NA values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T22:21:56.622754Z",
     "iopub.status.busy": "2021-07-20T22:21:56.622073Z",
     "iopub.status.idle": "2021-07-20T22:21:56.629958Z",
     "shell.execute_reply": "2021-07-20T22:21:56.630447Z",
     "shell.execute_reply.started": "2021-07-20T21:05:33.841771Z"
    },
    "papermill": {
     "duration": 0.05729,
     "end_time": "2021-07-20T22:21:56.630633",
     "exception": false,
     "start_time": "2021-07-20T22:21:56.573343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Checking for any na values\n",
    "all_data.isna().sum()\n",
    "#Result says we have none"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "\n",
    "There are no NA values in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step4: looking for outliers or miscoded values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T22:21:56.730448Z",
     "iopub.status.busy": "2021-07-20T22:21:56.729732Z",
     "iopub.status.idle": "2021-07-20T22:21:56.900461Z",
     "shell.execute_reply": "2021-07-20T22:21:56.899878Z",
     "shell.execute_reply.started": "2021-07-20T21:05:33.855793Z"
    },
    "papermill": {
     "duration": 0.225553,
     "end_time": "2021-07-20T22:21:56.900602",
     "exception": false,
     "start_time": "2021-07-20T22:21:56.675049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#looking for outliers or miscoded values\n",
    "all_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Let's take a look at the distribution of values, for the continuous variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = all_data.iloc[:,1:11]\n",
    "train_df.hist(figsize=(16,12),bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that the histogram of `Hillshade_3pm` contains several 0's, in the training set, which might make us think of missing values coded with 0, but according to the dataset description, this feaure can take value as 0, so these entries are valid.\n",
    "\n",
    "The next thing we should examine is the distribution of our variable of interest, 'Cover_Type' in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T22:21:56.997623Z",
     "iopub.status.busy": "2021-07-20T22:21:56.996951Z",
     "iopub.status.idle": "2021-07-20T22:21:57.376098Z",
     "shell.execute_reply": "2021-07-20T22:21:57.375557Z",
     "shell.execute_reply.started": "2021-07-20T21:05:34.039452Z"
    },
    "papermill": {
     "duration": 0.430587,
     "end_time": "2021-07-20T22:21:57.376256",
     "exception": false,
     "start_time": "2021-07-20T22:21:56.945669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEJCAYAAABohnsfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAa5klEQVR4nO3df1TUVeL/8dfILy1I0wZhWY95zF1OeBI3t6IM1laBUsKIClBZ8/TLLU23NEKUtDSXOFEeo+2cNT8n7duGHgV1DW1zsy08LbEnPbaulYolckZQVDCBYeb9/aPdWclW7pDDjPF8/AP3zh3nNT/kNe/3e37YLMuyBACAgT7+DgAAuHRQGgAAY5QGAMAYpQEAMEZpAACMURoAAGOUBgDAWLC/A/haU9MZud28FQUATPTpY9OVV17+P0//0ZeG221RGgBwkbB7CgBgjNIAABijNAAAxigNAIAxSgMAYIzSAAAYozQAAMZ+9O/T+K6IK/qqb1iIv2Ocp7XNqebTrV2uu7J/qIJDw3ogkXc62tvUdKq9y3VX9A9TWGhoDyTyTlt7u06farvgmgERoQrpG3i3vSQ5W9t0svnCt3//K/opNCww/8u3t3Xo1OmzF1zT/4pQhYYF3u3f3tamU6e7fuxf2b+fgkMD7/bvaO9Q06kL3/bnCrxr4GN9w0KUM/9Nf8c4z/8rmqJmdV0awaFhqil6oAcSeef6+X+U1PV/nLDQUE1f/bjvA3np/+5/WdKFSyOkb5i25t7fM4G8dMcbq6UuSiM0LFjLFqzvoUTeyV+a2eWa0LAwvfj0wz2Qxju/e/41mTz2g0ODtbv0fZ/n8dao3/7Kq/XsngIAGKM0AADGKA0AgDFKAwBgjNIAABijNAAAxigNAIAxSgMAYIzSAAAYozQAAMZ8WhorV67UxIkTNXHiRBUVFUmSqqqqlJaWpuTkZJWUlHjW7tu3TxkZGUpJSdGCBQvU0dEhSTp69KimTJmi1NRUzZw5U2fOnPFlZADABfisNKqqqvThhx9q48aNKi8v12effaYtW7YoPz9fpaWl2rp1q/bu3audO3dKkubNm6dFixZp27ZtsixLZWVlkqTFixcrJydHlZWVGjlypEpLS30VGQDQBZ+Vht1uV15enkJDQxUSEqLhw4ertrZWQ4cO1ZAhQxQcHKy0tDRVVlaqrq5Ora2tio+PlyRlZGSosrJSTqdT1dXVSklJ6TQPAPAPn5XGiBEjPCVQW1urd955RzabTXa73bMmMjJSDodDx44d6zRvt9vlcDjU1NSk8PBwBQcHd5oHAPiHzz8a/YsvvtDDDz+s+fPnKygoSLW1tZ7TLMuSzWaT2+2WzWY7b/4/P8/13XFXBg0K/0H5e5LdHuHvCD8I+f2L/P5zKWeXvMvv09KoqanR7NmzlZ+fr4kTJ+rvf/+7GhoaPKc3NDQoMjJSUVFRneYbGxsVGRmpgQMHqrm5WS6XS0FBQZ713jh+vEVut+UZB/Kd29DQ3OUa8vtOV/kDObtEfn/6MT32+/SxXfDJts92T9XX1+vRRx9VcXGxJk6cKEkaNWqUDh06pMOHD8vlcmnLli1KTExUTEyMwsLCVFNTI0mqqKhQYmKiQkJCNGbMGG3dulWSVF5ersTERF9FBgB0wWdbGqtWrVJbW5uWL1/umcvKytLy5cs1a9YstbW1KSkpSampqZKk4uJiFRQUqKWlRXFxccrNzZUkFRYWKi8vT6+++qqio6P14osv+ioyAKALPiuNgoICFRQUfO9pmzZtOm8uNjZW69ef/1WUMTExWrNmzUXPBwDwHu8IBwAYozQAAMYoDQCAMUoDAGCM0gAAGKM0AADGKA0AgDFKAwBgjNIAABijNAAAxigNAIAxSgMAYIzSAAAYozQAAMYoDQCAMUoDAGCM0gAAGKM0AADGKA0AgDFKAwBgjNIAABijNAAAxigNAIAxSgMAYIzSAAAYozQAAMYoDQCAMUoDAGCM0gAAGKM0AADGKA0AgDFKAwBgjNIAABijNAAAxigNAIAxSgMAYIzSAAAYozQAAMYoDQCAMZ+WRktLiyZNmqQjR45Ikp5++mklJycrPT1d6enpevfddyVJ+/btU0ZGhlJSUrRgwQJ1dHRIko4ePaopU6YoNTVVM2fO1JkzZ3wZFwDQBZ+Vxu7du5Wdna3a2lrP3N69e7V27VpVVFSooqJCEyZMkCTNmzdPixYt0rZt22RZlsrKyiRJixcvVk5OjiorKzVy5EiVlpb6Ki4AwIDPSqOsrEyFhYWKjIyUJJ09e1ZHjx5Vfn6+0tLStGLFCrndbtXV1am1tVXx8fGSpIyMDFVWVsrpdKq6ulopKSmd5gEA/hPsq3946dKlncaNjY266aabVFhYqIiICD388MNav369RowYIbvd7llnt9vlcDjU1NSk8PBwBQcHd5r31qBB4T/sivQguz3C3xF+EPL7F/n951LOLnmX32el8V1DhgzRK6+84hlPmzZN5eXlGj58uGw2m2fesizZbDbPz3N9d2zi+PEWud2WZxzId25DQ3OXa8jvO13lD+TsEvn96cf02O/Tx3bBJ9s99uqp/fv3a9u2bZ6xZVkKDg5WVFSUGhoaPPONjY2KjIzUwIED1dzcLJfLJUlqaGjw7OoCAPhHj5WGZVlatmyZTp06JafTqbffflsTJkxQTEyMwsLCVFNTI0mqqKhQYmKiQkJCNGbMGG3dulWSVF5ersTExJ6KCwD4Hj22eyo2NlYPPfSQsrOz1dHRoeTkZE2aNEmSVFxcrIKCArW0tCguLk65ubmSpMLCQuXl5enVV19VdHS0XnzxxZ6KCwD4Hj4vjR07dnh+nzJliqZMmXLemtjYWK1fv/68+ZiYGK1Zs8an+QAA5nhHOADAGKUBADBGaQAAjFEaAABjlAYAwBilAQAwRmkAAIxRGgAAY5QGAMAYpQEAMEZpAACMURoAAGOUBgDAGKUBADBGaQAAjFEaAABjlAYAwJhRaTgcjvPmvvzyy4seBgAQ2C5YGidPntTJkyf14IMP6tSpU55xY2OjHnvssZ7KCAAIEBf8jvAnnnhCH330kSTpxhtv/O+ZgoOVkpLi22QAgIBzwdJYtWqVJOnpp5/W888/3yOBAACB64Kl8R/PP/+86urqdOrUKVmW5ZmPi4vzWTAAQOAxKo0VK1Zo1apVGjRokGfOZrPpvffe81kwAEDgMSqN8vJybd++XYMHD/Z1HgBAADN6yW10dDSFAQAw29JISEhQUVGRfv3rX6tv376eeY5pAEDvYlQaGzZskCRVVlZ65jimAQC9j1Fp7Nixw9c5AACXAKPSWL169ffO33///Rc1DAAgsBmVxueff+75vb29XdXV1UpISPBZKABAYDJ+c9+5HA6HFixY4JNAAIDA1a2PRh88eLDq6uoudhYAQIDz+piGZVnau3dvp3eHAwB6B6+PaUjfvtlv/vz5PgkEAAhcXh3TqKurU0dHh4YOHerTUACAwGRUGocPH9Zvf/tbHTt2TG63W1deeaVee+01DR8+3Nf5AAABxOhA+JIlS/TAAw+ourpaNTU1mjlzphYvXuzrbACAAGNUGsePH9ddd93lGd99991qamryWSgAQGAyKg2Xy6WTJ096xidOnPBZIABA4DIqjalTp+q+++7TSy+9pJdfflnZ2dnKzs7u8nwtLS2aNGmSjhw5IkmqqqpSWlqakpOTVVJS4lm3b98+ZWRkKCUlRQsWLFBHR4ck6ejRo5oyZYpSU1M1c+ZMnTlzpjvXEQBwkRiVRlJSkiTJ6XTqwIEDcjgcmjBhwgXPs3v3bmVnZ6u2tlaS1Nraqvz8fJWWlmrr1q3au3evdu7cKUmaN2+eFi1apG3btsmyLJWVlUmSFi9erJycHFVWVmrkyJEqLS3t7vUEAFwERqWRl5enKVOmaN68eXrhhRc0Z84c5efnX/A8ZWVlKiwsVGRkpCRpz549Gjp0qIYMGaLg4GClpaWpsrJSdXV1am1tVXx8vCQpIyNDlZWVcjqdqq6uVkpKSqd5AID/GL3ktqmpSbm5uZKksLAwTZ8+XeXl5Rc8z9KlSzuNjx07Jrvd7hlHRkbK4XCcN2+32+VwONTU1KTw8HAFBwd3mvfWoEHhXp/HX+z2CH9H+EHI71/k959LObvkXX6j0nC5XHI4HJ6vfG1sbJRlWV6FcrvdstlsnrFlWbLZbP9z/j8/z/XdsYnjx1vkdv83ayDfuQ0NzV2uIb/vdJU/kLNL5PenH9Njv08f2wWfbBuVxvTp0zV58mTdeuutstlsqqqq8vpjRKKiotTQ0HBOyAZFRkaeN9/Y2KjIyEgNHDhQzc3NcrlcCgoK8qwHAPiP0TGNzMxMrV69Wtdee61GjhypVatWKS0tzasLGjVqlA4dOqTDhw/L5XJpy5YtSkxMVExMjMLCwlRTUyNJqqioUGJiokJCQjRmzBht3bpVklReXq7ExEQvrx4A4GIy2tKQpNjYWMXGxnb7gsLCwrR8+XLNmjVLbW1tSkpKUmpqqiSpuLhYBQUFamlpUVxcnOf4SWFhofLy8vTqq68qOjpaL774YrcvHwDwwxmXRned+/3iCQkJ2rRp03lrYmNjtX79+vPmY2JitGbNGp/mAwCY69aXMAEAeidKAwBgjNIAABijNAAAxigNAIAxSgMAYIzSAAAYozQAAMYoDQCAMUoDAGCM0gAAGKM0AADGKA0AgDFKAwBgjNIAABijNAAAxigNAIAxSgMAYIzSAAAYozQAAMYoDQCAMUoDAGCM0gAAGKM0AADGKA0AgDFKAwBgjNIAABijNAAAxigNAIAxSgMAYIzSAAAYozQAAMYoDQCAMUoDAGCM0gAAGKM0AADGKA0AgDFKAwBgjNIAABgL9seFTps2TSdOnFBw8LcXv2TJEp05c0bPP/+82tradPvtt2vu3LmSpH379mnBggU6c+aMxowZo8WLF3vOBwDoWT3+19eyLNXW1uqvf/2r549/a2urUlNTtWbNGkVHR+vhhx/Wzp07lZSUpHnz5um5555TfHy88vPzVVZWppycnJ6ODQCQH3ZPHTx4UJI0Y8YM3XnnnVq7dq327NmjoUOHasiQIQoODlZaWpoqKytVV1en1tZWxcfHS5IyMjJUWVnZ05EBAP/W41sap0+fVkJCghYuXCin06nc3Fw98MADstvtnjWRkZFyOBw6duxYp3m73S6Hw+HV5Q0aFH7Rsvua3R7h7wg/CPn9i/z+cylnl7zL3+OlMXr0aI0ePdozzszM1IoVK3T99dd75izLks1mk9vtls1mO2/eG8ePt8jttjzjQL5zGxqau1xDft/pKn8gZ5fI708/psd+nz62Cz7Z7vHdU5988ol27drlGVuWpZiYGDU0NHjmGhoaFBkZqaioqE7zjY2NioyM7NG8AID/6vHSaG5uVlFRkdra2tTS0qKNGzfqd7/7nQ4dOqTDhw/L5XJpy5YtSkxMVExMjMLCwlRTUyNJqqioUGJiYk9HBgD8W4/vnho3bpx2796tyZMny+12KycnR6NHj9by5cs1a9YstbW1KSkpSampqZKk4uJiFRQUqKWlRXFxccrNze3pyACAf/PLGx7mzJmjOXPmdJpLSEjQpk2bzlsbGxur9evX91Q0AMAF8I5wAIAxSgMAYIzSAAAYozQAAMYoDQCAMUoDAGCM0gAAGKM0AADGKA0AgDFKAwBgjNIAABijNAAAxigNAIAxSgMAYIzSAAAYozQAAMYoDQCAMUoDAGCM0gAAGKM0AADGKA0AgDFKAwBgjNIAABijNAAAxigNAIAxSgMAYIzSAAAYozQAAMYoDQCAMUoDAGCM0gAAGKM0AADGKA0AgDFKAwBgjNIAABijNAAAxigNAIAxSgMAYIzSAAAYuyRKY/PmzbrjjjuUnJysN998099xAKDXCvZ3gK44HA6VlJRow4YNCg0NVVZWlm688UZdc801/o4GAL1OwJdGVVWVbrrpJg0YMECSlJKSosrKSj322GNG5+/Tx3be3FVXXn5RM14s35f1+4ReMcjHSbrHNP9V4QN9nKR7TPL3uyowb3vJLH//AZf1QJLuMcl/xYDAvP1NH/shEX19nKR7zs3f1XWxWZZl+TrQD/Haa6/pm2++0dy5cyVJ69at0549e/Tss8/6ORkA9D4Bf0zD7XbLZvtv81mW1WkMAOg5AV8aUVFRamho8IwbGhoUGRnpx0QA0HsFfGncfPPN2rVrl06cOKGzZ89q+/btSkxM9HcsAOiVAv5A+ODBgzV37lzl5ubK6XQqMzNT1113nb9jAUCvFPAHwgEAgSPgd08BAAIHpQEAMEZpAACMURoAAGMB/+qpQNXS0qKsrCz94Q9/0E9/+lN/x/HKypUr9c4770iSkpKSNH/+fD8n8s7LL7+sbdu2yWazKTMzU/fff7+/I3XL73//ezU1NWn58uX+juKVadOm6cSJEwoO/vbPx5IlSzRq1Cg/pzK3Y8cOrVy5UmfPntUtt9yigoICf0cytm7dOq1du9YzPnLkiNLT07Vo0aKeC2HBa59++qk1adIkKy4uzvr666/9HccrH330kXXfffdZbW1tVnt7u5Wbm2tt377d37GMffzxx1ZWVpbldDqts2fPWuPGjbMOHDjg71heq6qqsm688Ubrqaee8ncUr7jdbmvs2LGW0+n0d5Ru+eqrr6yxY8da9fX1Vnt7u5WdnW29//77/o7VLZ9//rk1YcIE6/jx4z16ueye6oaysjIVFhZeku9Mt9vtysvLU2hoqEJCQjR8+HAdPXrU37GM3XDDDXrjjTcUHBys48ePy+Vy6bLLAvdD+L7PyZMnVVJSokceecTfUbx28OBBSdKMGTN05513dnrWeyl49913dccddygqKkohISEqKSm5pLaSzvXMM89o7ty5GjiwZz8AlN1T3bB06VJ/R+i2ESNGeH6vra3VO++8o7feesuPibwXEhKiFStW6PXXX1dqaqoGDx7s70heWbRokebOnav6+np/R/Ha6dOnlZCQoIULF8rpdCo3N1fDhg3TLbfc4u9oRg4fPqyQkBA98sgjqq+v169+9SvNmTPH37G8VlVVpdbWVt1+++09ftlsafRSX3zxhWbMmKH58+fr6quv9nccr82ePVu7du1SfX29ysrK/B3H2Lp16xQdHa2EhAR/R+mW0aNHq6ioSBERERo4cKAyMzO1c+dOf8cy5nK5tGvXLi1btkxvv/229uzZo40bN/o7ltf+9Kc/+e1YHqXRC9XU1Gj69Ol64okndNddd/k7jlcOHDigffv2SZL69eun5ORk7d+/38+pzG3dulUfffSR0tPTtWLFCu3YsUPLli3zdyxjn3zyiXbt2uUZW5blOSB+KbjqqquUkJCggQMHqm/fvho/frz27Nnj71heaW9vV3V1tW677Ta/XD6l0cvU19fr0UcfVXFxsSZOnOjvOF47cuSICgoK1N7ervb2dr333nu6/vrr/R3L2OrVq7VlyxZVVFRo9uzZuu2225Sfn+/vWMaam5tVVFSktrY2tbS0aOPGjZowYYK/YxkbN26cPvzwQ50+fVoul0t/+9vfFBcX5+9YXtm/f7+uvvpqvx3Lu3SeIuCiWLVqldra2jq9zDMrK0vZ2dl+TGUuKSlJe/bs0eTJkxUUFKTk5ORLsvwuVePGjdPu3bs1efJkud1u5eTkaPTo0f6OZWzUqFF64IEHlJOTI6fTqVtuuUV33323v2N55euvv1ZUVJTfLp8PLAQAGGP3FADAGKUBADBGaQAAjFEaAABjlAYAwBgvuUWv5nK59MYbb2jz5s1yuVxyOp0aN26cHn/8cYWGhvZ4ntOnT2vatGmSpG+++UYOh0PDhg2TJN1888166qmnejwTcC5ecotebeHChTp16pSWLl2qiIgIffPNN3ryySd1+eWX64UXXvBrto8//ljPPvustmzZ4tccwLnYPYVe68iRI9q8ebOWLVumiIgISdJll12mxYsXa/z48WpubtaTTz6pSZMmKS0tTUVFRero6NDbb7/d6RNqDxw4oFtvvVUul0sHDhzQjBkzlJGRofT0dK1fv17StwVw5513KisrS2lpaWpvb/c676ZNm5SVleUZHz16VGPHjlV7e7uuvfZalZSUKCMjQ6mpqdq+fbtn3bp165SRkaHJkydr+vTpOnDgQHdvMoDdU+i9PvvsM11zzTUKDw/vNG+325WSkqKnnnpKAwYM0ObNm+V0OjVz5ky9/vrrysnJUXFxsRoaGmS327VhwwZlZGTIsizNnj1bRUVFiouLU3Nzs+677z5dc801kr79kMi//OUviomJ6Vbe1NRULV++XF988YVGjBihdevW6a677lJoaKhcLpf69eunDRs26F//+pemTp2qMWPG6Msvv1R5ebnefPNN9evXTx9++KEee+wxz5dwAd5iSwO9Vp8+feR2u//n6R988IGmTp0qm82m0NBQZWVl6YMPPlB4eLgmTJigTZs2yeVyafPmzcrMzFRtba2++uor5efnKz09XVOnTlVra6v++c9/SpKio6O7XRiSFBoaqnvuuUfr1q2Ty+XSxo0bde+993pOnzp1qiQpNjZWP/vZz1RdXa33339fhw8fVlZWltLT0/XCCy/o9OnTOnnyZLdzoHdjSwO91nXXXaeDBw+qpaWl09aGw+HQwoUL5Xa7ZbPZPPNut1sdHR2SpHvvvVcLFy7U8OHDNXz4cA0ZMkT79+9XRESEKioqPOdpbGxURESEPv3004vyAXNZWVnKzMzUDTfcoBEjRmjIkCGe04KCgjplDQoKktvtVnp6uubNm+eZP3bsmPr37/+Ds6B3YksDvdbgwYOVlpam/Px8tbS0SPr2u9+feeYZDRgwQGPHjtXatWtlWZba29tVVlamm2++WZIUHx8vSXrllVd0zz33SJKGDRumvn37ekqjvr5ekyZN0t69ey9a5ujoaMXHx2vZsmXnfchkeXm5pG93ux06dEi//OUvNXbsWP35z3/WsWPHJElvvfWWfvOb31y0POh92NJAr1ZYWKjS0lJlZWUpKChI7e3tGj9+vGbNmqUzZ87oueeeU1pampxOp2699dZOB8DvuecelZaWavz48ZK+3X1UWlqqpUuX6o9//KM6Ojr0+OOP6/rrr9fHH3980TJnZGTo2WefVVJSUqf5f/zjHyorK5Pb7VZJSYn69++vsWPH6sEHH9SMGTNks9kUHh6ulStXdtqCArzBS26BS4jb7daSJUv0k5/8RA899JBn/uc//7l27drV498Xjd6HLQ2ghx08eFBz58793tOGDRuml1566XtPa2lp0bhx4/SLX/xCeXl5vowI/E9saQAAjHEgHABgjNIAABijNAAAxigNAIAxSgMAYIzSAAAY+//Q8RIeGU/UpQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "ax = sns.countplot(x=\"Cover_Type\", data=all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there is a perfectly even distribution of 'Cover_Type' in our data. This is ideal, as it means we have an equal number of examples for each of our data types, allowing us to avoid a situation where one type in particular is predicted poorly due to a lack of examples.\n",
    "\n",
    "The next thing for us to examine is the distribution of our feature variables across the target variable. This should allow us some insight into which variables might be useful for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T22:21:57.497906Z",
     "iopub.status.busy": "2021-07-20T22:21:57.497279Z",
     "iopub.status.idle": "2021-07-20T22:22:00.293066Z",
     "shell.execute_reply": "2021-07-20T22:22:00.293590Z",
     "shell.execute_reply.started": "2021-07-20T21:05:34.430286Z"
    },
    "papermill": {
     "duration": 2.871457,
     "end_time": "2021-07-20T22:22:00.293762",
     "exception": false,
     "start_time": "2021-07-20T22:21:57.422305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Next lets look at the distribution of the numeric variables across cover types.\n",
    "plotcount=10\n",
    "fig, axes = plt.subplots(nrows = 5,ncols = 2,figsize = (25,40))\n",
    "for i in range(0,plotcount):\n",
    "    row = i // 2\n",
    "    col = i % 2\n",
    "    ax1 = axes[row, col]\n",
    "    sns.boxplot(x=\"Cover_Type\", y=all_data.columns[i+1], data=all_data,ax=ax1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the boxplots above, many of our continuous variables have a very similar distribution across cover types. The only one that stands out by eye is \"Elevation\", which appears to vary quite differently by each cover type.\n",
    "\n",
    "Another important piece to examine is the correlation of each of the continuous explanatory variables with each other. Too high of a correlation, or even a perfect correlation, between too many features could be problematic for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T22:22:00.401502Z",
     "iopub.status.busy": "2021-07-20T22:22:00.400798Z",
     "iopub.status.idle": "2021-07-20T22:22:00.871106Z",
     "shell.execute_reply": "2021-07-20T22:22:00.871692Z",
     "shell.execute_reply.started": "2021-07-20T21:05:37.27732Z"
    },
    "papermill": {
     "duration": 0.527049,
     "end_time": "2021-07-20T22:22:00.871883",
     "exception": false,
     "start_time": "2021-07-20T22:22:00.344834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#let's look at the numeric predictor variables and see which are correlated\n",
    "correlations = all_data.iloc[:,1:11].corr()\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "sns.heatmap(correlations, vmax=.8, square=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are some variables that are highly correlated. Many of them make intuitive sense. For example, Slope and Hillshade at noon are highly correlated. This makes sense, as the slope of a hill will have a direct impact on the shade when the sun is directly overhead. Likewise, Aspect (also known as the direction the hill is facing) is highly correlated with shade in the morning and evening. When the sun is low in the sky on either side, east or west facing slopes will have very different amounts of shade.\n",
    "\n",
    "Luckily for us it appears that nothing has a perfect correlation, which could adversely affect something like a regression model, but the high correlation between these variables may make them less useful when fitting our classifiers.\n",
    "\n",
    "Finally we need to look at our categorical variables. We will plot each of the 3 groups of categorical variables (Cover Type, Soil Type and Wilderness Area) against each other to look for any patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T22:22:00.989337Z",
     "iopub.status.busy": "2021-07-20T22:22:00.988646Z",
     "iopub.status.idle": "2021-07-20T22:22:04.004655Z",
     "shell.execute_reply": "2021-07-20T22:22:04.005161Z",
     "shell.execute_reply.started": "2021-07-20T21:05:37.730818Z"
    },
    "papermill": {
     "duration": 3.079612,
     "end_time": "2021-07-20T22:22:04.005351",
     "exception": false,
     "start_time": "2021-07-20T22:22:00.925739",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "soil_type = all_data.iloc[:,15:55]\n",
    "soil_type = pd.DataFrame(soil_type)\n",
    "soil_col = pd.Series(soil_type.columns[np.where(soil_type == 1)[1]])\n",
    "print(soil_col)\n",
    "\n",
    "wild_type = all_data.iloc[:,11:15]\n",
    "wild_type = pd.DataFrame(wild_type)\n",
    "wild_col = pd.Series(wild_type.columns[np.where(wild_type == 1)[1]])\n",
    "\n",
    "import copy\n",
    "\n",
    "plot_data = copy.deepcopy(all_data)\n",
    "plot_data['soil'] = soil_col\n",
    "plot_data['wilderness'] = wild_col\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 3,ncols = 1,figsize = (25,45))\n",
    "\n",
    "ax1 = axes[0]\n",
    "pd.crosstab(plot_data.soil, plot_data.wilderness).plot.bar(stacked = True, ax = ax1)\n",
    "\n",
    "ax2 = axes[1]\n",
    "pd.crosstab(plot_data.wilderness, plot_data.Cover_Type).plot.bar(stacked = True, ax = ax2)\n",
    "\n",
    "ax3 = axes[2]\n",
    "pd.crosstab(plot_data.soil, plot_data.Cover_Type).plot.bar(stacked = True, ax = ax3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are absolutely some striking and promising patterns in this data. Firstly we see that some soil types and wilderness areas are mutually exclusive, with certain soils only appearing in certain areas. Likewise, we see that certain cover types are completely absent from certain soil types or wilderness areas. This should mean that these variables will be strongly indicative of what cover type we can expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In order to make continuous variable comparable to the binary features, we will be using `preprocessing.MinMaxScaler`, to standardize them by scaling each feature to a given range; [0,1].\n",
    "This can help in `Logistic regression` model and `kNN` model.\n",
    "\n",
    "We also use `preprocessing.StandardScaler`, to standardize 10 continuous feature by moving the mean and scaling to unit variance),\n",
    "\n",
    "With scaled features, there is no surity that the model results will improve, although it is mandatory in this case, since the dataset is a combination of continuous and binary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale to mean = 0, sd = 1 using StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scaler = preprocessing.StandardScaler()\n",
    "\n",
    "train_data_std = copy.deepcopy(train_data)\n",
    "dev_data_std = copy.deepcopy(dev_data)\n",
    "#test_data_std = copy.deepcopy(test_data)\n",
    "\n",
    "# only for the continuous features (first 10 columns in the train_data, dev_data and test_data)\n",
    "train_data_std.iloc[:, :10] = std_scaler.fit_transform(train_data_std.iloc[:, :10])\n",
    "dev_data_std.iloc[:, :10] = std_scaler.transform(dev_data_std.iloc[:, :10])\n",
    "#test_data_std.iloc[:, :10] = std_scaler.transform(test_data_std.iloc[:, :10])\n",
    "\n",
    "# Scale to range [0,1] using MinMaxScaler\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "train_data_minmax = copy.deepcopy(train_data)\n",
    "dev_data_minmax = copy.deepcopy(dev_data)\n",
    "#test_data_minmax = copy.deepcopy(test_data)\n",
    "\n",
    "# Only for the continuous features (first 10 columns in the train_data, dev_data and test_data)\n",
    "train_data_minmax.iloc[:, :10] = min_max_scaler.fit_transform(train_data.iloc[:, :10])\n",
    "dev_data_minmax.iloc[:, :10] = min_max_scaler.transform(dev_data.iloc[:, :10])\n",
    "#test_data_minmax.iloc[:, :10] = min_max_scaler.transform(test_data.iloc[:, :10])\n",
    "\n",
    "# Feature Binarization is the process of thresholding numerical features to get boolean values,(first 10 columns in the train_data, dev_data and test_data, since they are non-binary features)\n",
    "# Binarize feature values to either 0 or 1, using binarizer\n",
    "binarizer = preprocessing.Binarizer()\n",
    "\n",
    "train_data_b = copy.deepcopy(train_data)\n",
    "dev_data_b = copy.deepcopy(dev_data)\n",
    "#test_data_b = copy.deepcopy(test_data)\n",
    "\n",
    "# Only for the continuous features (first 10 columns in the train_data, dev_data and test_data)\n",
    "train_data_b.iloc[:, :10] = binarizer.fit_transform(train_data.iloc[:, :10])\n",
    "dev_data_b.iloc[:, :10] = binarizer.transform(dev_data.iloc[:, :10])\n",
    "#test_data_b.iloc[:, :10] = binarizer.transform(test_data.iloc[:, :10])\n",
    "\n",
    "print(train_data_b.shape)\n",
    "#print(train_data_b.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 : Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.058649,
     "end_time": "2021-07-20T22:22:09.360302",
     "exception": false,
     "start_time": "2021-07-20T22:22:09.301653",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model 1: \n",
    "## k-Nearest Neighbors(kNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.057232,
     "end_time": "2021-07-20T22:22:09.475533",
     "exception": false,
     "start_time": "2021-07-20T22:22:09.418301",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's run an experiment with Nearest Neighbors classifier, using fit() and predict() methods from the sklearn classifier implementations.\n",
    "\n",
    "Let's start by finding the optimal k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T22:22:09.596711Z",
     "iopub.status.busy": "2021-07-20T22:22:09.596009Z",
     "iopub.status.idle": "2021-07-20T22:22:32.275583Z",
     "shell.execute_reply": "2021-07-20T22:22:32.276358Z",
     "shell.execute_reply.started": "2021-07-20T21:05:46.131305Z"
    },
    "papermill": {
     "duration": 22.743675,
     "end_time": "2021-07-20T22:22:32.276587",
     "exception": false,
     "start_time": "2021-07-20T22:22:09.532912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Estimate by cross-validation of the optimal number of neighbors (k)\n",
    "# Try between 1 and the number of features (54)\n",
    "#k = {'n_neighbors': np.concatenate([np.arange(1, train_data.shape[1]+1)]).tolist()}\n",
    "# The optimal value is low, so let's narrow the search from 1 to 11\n",
    "k = {'n_neighbors': np.concatenate([np.arange(1, 10+1)]).tolist()}\n",
    "best_param_kNN = GridSearchCV(KNeighborsClassifier(), k, scoring='accuracy')\n",
    "best_param_kNN.fit(train_data, train_label)\n",
    "optimal_k = best_param_kNN.best_params_['n_neighbors']\n",
    "print ('The optimal value for k is {0}'.format(optimal_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T22:22:32.408395Z",
     "iopub.status.busy": "2021-07-20T22:22:32.407326Z",
     "iopub.status.idle": "2021-07-20T22:22:34.777124Z",
     "shell.execute_reply": "2021-07-20T22:22:34.776565Z",
     "shell.execute_reply.started": "2021-07-20T21:06:08.934275Z"
    },
    "papermill": {
     "duration": 2.440172,
     "end_time": "2021-07-20T22:22:34.777273",
     "exception": false,
     "start_time": "2021-07-20T22:22:32.337101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# first approach of finding kNN with different data manipulations\n",
    "kNN = KNeighborsClassifier(n_neighbors=optimal_k)\n",
    "\n",
    "kNN.fit(train_data, train_label)\n",
    "print ('Accuracy using non-scaled data:      {0:.4f}'.format(kNN.score(dev_data, dev_label)))\n",
    "\n",
    "kNN.fit(train_data_std, train_label)\n",
    "print ('Accuracy using standardized data:    {0:.4f}'.format(kNN.score(dev_data_std, dev_label)))\n",
    "\n",
    "kNN.fit(train_data_minmax, train_label)\n",
    "print ('Accuracy using scaled-to-range data: {0:.4f}'.format(kNN.score(dev_data_minmax, dev_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.057472,
     "end_time": "2021-07-20T22:22:34.893366",
     "exception": false,
     "start_time": "2021-07-20T22:22:34.835894",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As we can see, the model performed better with non-scaled data, although the optimal k was searched above using non-scaled data, if we calculate optimal-k even with standardized or scale_to_range data, it gives the same result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T22:22:35.019526Z",
     "iopub.status.busy": "2021-07-20T22:22:35.018810Z",
     "iopub.status.idle": "2021-07-20T22:22:36.271291Z",
     "shell.execute_reply": "2021-07-20T22:22:36.270687Z",
     "shell.execute_reply.started": "2021-07-20T21:06:11.374807Z"
    },
    "papermill": {
     "duration": 1.320102,
     "end_time": "2021-07-20T22:22:36.271431",
     "exception": false,
     "start_time": "2021-07-20T22:22:34.951329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# second approach of finding kNN without finding the optimal value first\n",
    "#Here are our K values to try\n",
    "k_values = [1, 3, 5, 7, 9]\n",
    "\n",
    "for i in k_values:\n",
    "    clf = KNeighborsClassifier(n_neighbors = i)\n",
    "    clf.fit(train_data.iloc[:, :10], train_label)\n",
    "    #clf.fit(train_data, train_label)\n",
    "    # Predict on the dev data\n",
    "    preds = clf.predict(dev_data.iloc[:, :10])\n",
    "    #preds = clf.predict(dev_data)\n",
    "\n",
    "    # And calculate the accuracy by comparing it to the labels\n",
    "    correct, total = 0, 0\n",
    "    for pred, label in zip(preds, dev_label):\n",
    "        if pred == label: \n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    # Finally we print the outcomes\n",
    "    Outcome = [\"For the model with\", str(i), \"Nearest Neighbors:\"]\n",
    "    print(\" \".join(Outcome))\n",
    "    print ('total: %3d  correct: %3d  accuracy: %3.2f' %(total, correct, 1.0*correct/total))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will proceed with the accuracy result from kNN as our baseline and will try to meet or improve the baseline using different classifiers below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.05912,
     "end_time": "2021-07-20T22:22:36.390125",
     "exception": false,
     "start_time": "2021-07-20T22:22:36.331005",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model 2:\n",
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.059481,
     "end_time": "2021-07-20T22:22:36.509059",
     "exception": false,
     "start_time": "2021-07-20T22:22:36.449578",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Decision trees are powerful machine learning algorithms that iteratively split the data based on binary 'Yes/No' decision criterion. This has the advantage of both working well with data that is already highly binary, such as our \"Wilderness Area\" and \"Soil Type\" variables, and being explainable to non-technical audiences. \n",
    "\n",
    "In order to properly utilize the Decision Tree Algorithm, our first task is to binarize all the columsn in the data. For most of our variables, this is fairly easy, as they are already structured as binary indicator variables. For our remaining numeric variables, we need to look at the distribution of our data to see if there are natural breakpoints we can utilize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T22:22:36.691552Z",
     "iopub.status.busy": "2021-07-20T22:22:36.670804Z",
     "iopub.status.idle": "2021-07-20T22:22:39.993949Z",
     "shell.execute_reply": "2021-07-20T22:22:39.994460Z",
     "shell.execute_reply.started": "2021-07-20T21:06:12.641151Z"
    },
    "papermill": {
     "duration": 3.425706,
     "end_time": "2021-07-20T22:22:39.994622",
     "exception": false,
     "start_time": "2021-07-20T22:22:36.568916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating our plot grid\n",
    "plotcount=10\n",
    "fig, axes = plt.subplots(nrows = 5,ncols = 2,figsize = (25,40))\n",
    "#Plotting our variables\n",
    "for i in range(0,plotcount):\n",
    "    row = i // 2\n",
    "    col = i % 2\n",
    "    ax1 = axes[row, col]\n",
    "    sns.histplot(y=all_data.columns[i+1], data=all_data,ax=ax1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.081457,
     "end_time": "2021-07-20T22:22:40.139924",
     "exception": false,
     "start_time": "2021-07-20T22:22:40.058467",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "It looks like some of our variables have obvious breakpoints, while others have a fairly smooth distribution.\n",
    "\n",
    "The variables with obvious breakpoints are:\n",
    "* Elevation: Elevation looks to have 3 groups, one from 0 to about 2625, one from 2625 to 3125 and one for 3125 and above\n",
    "* Aspect: Aspect seems to be split into two groups right at 250\n",
    "* Slope: Slope has a gap at 26 degrees. We can try breaking here\n",
    "* Horizontal_Distance_To_Hydrology: This seems to have a large amount of data below 100 feet, and a long tail above that\n",
    "* Vertical_Distance_To_Hydrology: This feature seems to have an overwhelming amount of the data at exactly 0, so we will split it there\n",
    "\n",
    "The other variables: Horizontal_Distance_To_Roadways, the Hillshade variables, and Horizontal_Distance_To_Fire_Points seem to have a fairly smooth distribution. Luckily, since we know the criterion that a decision tree will split on (information gain), we can choose to binarize our continuous variables at those points which maximize this criterion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T22:22:40.281146Z",
     "iopub.status.busy": "2021-07-20T22:22:40.280504Z",
     "iopub.status.idle": "2021-07-20T22:22:40.285434Z",
     "shell.execute_reply": "2021-07-20T22:22:40.284911Z",
     "shell.execute_reply.started": "2021-07-20T21:22:58.6186Z"
    },
    "papermill": {
     "duration": 0.07419,
     "end_time": "2021-07-20T22:22:40.285576",
     "exception": false,
     "start_time": "2021-07-20T22:22:40.211386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#First we will create a copy of our data that we can modify\n",
    "tree_train_data = copy.deepcopy(train_data)\n",
    "tree_train_labels = copy.deepcopy(train_label)\n",
    "tree_dev_data = copy.deepcopy(dev_data)\n",
    "tree_dev_labels = copy.deepcopy(dev_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T22:22:40.458759Z",
     "iopub.status.busy": "2021-07-20T22:22:40.421421Z",
     "iopub.status.idle": "2021-07-20T22:23:52.153889Z",
     "shell.execute_reply": "2021-07-20T22:23:52.153335Z",
     "shell.execute_reply.started": "2021-07-20T21:20:16.165938Z"
    },
    "papermill": {
     "duration": 71.804693,
     "end_time": "2021-07-20T22:23:52.154068",
     "exception": false,
     "start_time": "2021-07-20T22:22:40.349375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "###This block of four functions work together to find the point each variable can be split to maximize information gain.\n",
    "\n",
    "def entropy(distribution):\n",
    "    #initialize entropy at zero\n",
    "    h = 0.0\n",
    "    for probability in distribution:\n",
    "        logprob = -100.0  # log(0) = -inf so let's approximate it with -100 to avoid an error\n",
    "        if probability > 0.0: logprob = np.log2(probability)\n",
    "        h -= probability * logprob\n",
    "    return h\n",
    "\n",
    "def get_label_distribution(labels):\n",
    "    # Initialize counters for all labels to zero.\n",
    "    label_probs = np.array([0.0 for i in range(7)])\n",
    "\n",
    "    # Iterate over labels in the training data and update counts.\n",
    "    for label in labels:\n",
    "        label_probs[label-1] += 1.0\n",
    "    \n",
    "    # Normalize to get a distribution.\n",
    "    label_probs /= label_probs.sum()\n",
    "    return label_probs\n",
    "\n",
    "def information_gain(data, labels, feature, threshold=0):\n",
    "    # Get the initial entropy of the label distribution.\n",
    "    initial_entropy = entropy(get_label_distribution(labels))\n",
    "    \n",
    "    # subset0 will contain the labels for which the feature is 0 and\n",
    "    # subset1 will contain the labels for which the feature is 1.\n",
    "    subset0, subset1 = [], []\n",
    "    for datum, label in zip(data, labels):\n",
    "        if datum[feature] > threshold: subset1.append(label)\n",
    "        else: subset0.append(label)\n",
    "    \n",
    "    # Compute the entropy of each subset.\n",
    "    subset0_entropy = entropy(get_label_distribution(subset0))\n",
    "    subset1_entropy = entropy(get_label_distribution(subset1))\n",
    "    \n",
    "    # Compute the final entropy by weighting each subset's entropy according to its size.\n",
    "    subset0_weight = 1.0 * len(subset0) / len(labels)\n",
    "    subset1_weight = 1.0 * len(subset1) / len(labels)\n",
    "    final_entropy = subset0_weight * subset0_entropy + subset1_weight * subset1_entropy\n",
    "    \n",
    "    # Finally, compute information gain as the difference between the initial and final entropy.\n",
    "    return initial_entropy - final_entropy\n",
    "\n",
    "def try_features_and_thresholds(x, y, emptylist = []):\n",
    "    #This function is what actually tries the different threshold values and chooses the best one\n",
    "    data = x.to_numpy()\n",
    "    labels = y.to_numpy()\n",
    "    for feature in range(data.shape[1]):\n",
    "        # Choose a set of thresholds between the min- and max-valued feature, ignoring the min and max themselves.\n",
    "        thresholds = np.linspace(data[:,feature].min(), data[:,feature].max(), 102)[1:-1]\n",
    "\n",
    "        # Try each threshold and keep track of the best one for this feature.\n",
    "        best_threshold = 0\n",
    "        best_ig = 0\n",
    "        for threshold in thresholds:\n",
    "            ig = information_gain(data, labels, feature, threshold)\n",
    "            if ig > best_ig:\n",
    "                best_ig = ig\n",
    "                best_threshold = threshold\n",
    "\n",
    "        # Show the best threshold and information gain for this feature.\n",
    "        print ('%d %.3f %.3f %s' %(feature, best_threshold, best_ig, x.columns[feature]))\n",
    "        emptylist.append(best_threshold)\n",
    "    return emptylist\n",
    "\n",
    "#Now we run it on our model to get the best thresholds to binarize each variable\n",
    "bestcutoffs = try_features_and_thresholds(tree_train_data.iloc[:,0:10], tree_train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've found the appropriate cutoffs for each of our continuous variables, we can turn them into binary indicators. It's important that we do this precisely and specifically, since we will need to binarize our test data exactly the same way (rather than by recalculating what the best entropy points are for the test data). This keeps us from incorrectly fitting our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T22:23:52.330701Z",
     "iopub.status.busy": "2021-07-20T22:23:52.329520Z",
     "iopub.status.idle": "2021-07-20T22:23:52.412504Z",
     "shell.execute_reply": "2021-07-20T22:23:52.412960Z",
     "shell.execute_reply.started": "2021-07-20T21:23:02.172051Z"
    },
    "papermill": {
     "duration": 0.179029,
     "end_time": "2021-07-20T22:23:52.413196",
     "exception": false,
     "start_time": "2021-07-20T22:23:52.234167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def binarize_data_for_tree(x):\n",
    "    #Now we will create our elevation features, dividing it up into two sections with two new column\n",
    "    x[\"Elevation_high\"] = np.where(x[\"Elevation\"] > bestcutoffs[0], 1, 0)\n",
    "    x.drop(\"Elevation\", axis = 1, inplace = True)\n",
    "\n",
    "    #Next we create our aspect feature\n",
    "    x[\"Aspect_high\"] = np.where(x[\"Aspect\"] > bestcutoffs[1] , 1, 0)\n",
    "    x.drop(\"Aspect\", axis = 1, inplace = True)\n",
    "\n",
    "    #Our slope feature\n",
    "    x[\"Slope_high\"] = np.where(x[\"Slope\"] > bestcutoffs[2], 1, 0)\n",
    "    x.drop(\"Slope\", axis = 1, inplace = True)\n",
    "\n",
    "    #Our Horizontal distance to hydrology feature\n",
    "    x[\"Horizon_Dist_Hydrology_high\"] = np.where(x[\"Horizontal_Distance_To_Hydrology\"] > bestcutoffs[3], 1, 0)\n",
    "    x.drop(\"Horizontal_Distance_To_Hydrology\", axis = 1, inplace = True)\n",
    "\n",
    "    #Our Vertical distance to hydrology feature\n",
    "    x[\"Vert_Dist_Hydrology_high\"] = np.where(x[\"Vertical_Distance_To_Hydrology\"] > bestcutoffs[4] , 1, 0)\n",
    "    x.drop(\"Vertical_Distance_To_Hydrology\", axis = 1, inplace = True)\n",
    "\n",
    "    #Variables split on the mean\n",
    "    x[\"Horizon_Dist_Road_high\"] = np.where(x[\"Horizontal_Distance_To_Roadways\"] > bestcutoffs[5], 1, 0)\n",
    "    x.drop(\"Horizontal_Distance_To_Roadways\", axis = 1, inplace = True)\n",
    "\n",
    "    x[\"Hillshade_9am_high\"] = np.where(x[\"Hillshade_9am\"] > bestcutoffs[6], 1, 0)\n",
    "    x.drop(\"Hillshade_9am\", axis = 1, inplace = True)\n",
    "\n",
    "    x[\"Hillshade_Noon_high\"] = np.where(x[\"Hillshade_Noon\"] > bestcutoffs[7], 1, 0)\n",
    "    x.drop(\"Hillshade_Noon\", axis = 1, inplace = True)\n",
    "\n",
    "    x[\"Hillshade_3pm_high\"] = np.where(x[\"Hillshade_3pm\"] > bestcutoffs[8], 1, 0)\n",
    "    x.drop(\"Hillshade_3pm\", axis = 1, inplace = True)\n",
    "\n",
    "    x[\"Horizon_Dist_Fire_high\"] = np.where(x[\"Horizontal_Distance_To_Fire_Points\"] > bestcutoffs[9], 1, 0)\n",
    "    x.drop(\"Horizontal_Distance_To_Fire_Points\", axis = 1, inplace = True)\n",
    "    \n",
    "\n",
    "binarize_data_for_tree(tree_train_data)\n",
    "binarize_data_for_tree(tree_dev_data)\n",
    "    \n",
    "#Checking our data\n",
    "print(tree_train_data.head())\n",
    "print(tree_dev_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.080419,
     "end_time": "2021-07-20T22:23:52.575775",
     "exception": false,
     "start_time": "2021-07-20T22:23:52.495356",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Once we have succressfully binarized our data, we can attempt to fit a single decision tree classifier to make predictions. The two main hyper parameters to be concerned with in a Decision Tree Classifier are the minimum number of samples we require in order to perform a split and the maximum depth of the tree. As noted in the KNN section, we can use the Grid Search Cross-Validation to choose the best parameters for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T22:23:52.748666Z",
     "iopub.status.busy": "2021-07-20T22:23:52.747562Z",
     "iopub.status.idle": "2021-07-20T22:23:53.233930Z",
     "shell.execute_reply": "2021-07-20T22:23:53.234512Z",
     "shell.execute_reply.started": "2021-07-20T21:24:16.816998Z"
    },
    "papermill": {
     "duration": 0.577783,
     "end_time": "2021-07-20T22:23:53.234696",
     "exception": false,
     "start_time": "2021-07-20T22:23:52.656913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Setting up our grid search\n",
    "paramsDT = {'min_samples_split': np.concatenate([np.arange(2, 10+1)]).tolist(),\n",
    "           'max_depth': [int(x) for x in np.linspace(1, 101, num = 20)]}\n",
    "best_param_DT = GridSearchCV(DecisionTreeClassifier(), paramsDT, scoring='accuracy')\n",
    "best_param_DT.fit(tree_train_data, tree_train_labels)\n",
    "DT_optimal_sample_param = best_param_DT.best_params_['min_samples_split']\n",
    "DT_optimal_depth_param = best_param_DT.best_params_['max_depth']\n",
    "#Displaying our best parameter\n",
    "print ('The optimal value for min_samples_split is {0}'.format(DT_optimal_sample_param))\n",
    "print ('The optimal value for max_depth is {0}'.format(DT_optimal_depth_param))\n",
    "\n",
    "#Fitting our classifier with the best parameter\n",
    "clf = DecisionTreeClassifier(criterion='entropy', min_samples_split=DT_optimal_sample_param, \n",
    "                             max_depth = DT_optimal_depth_param)\n",
    "clf.fit(tree_train_data, tree_train_labels)\n",
    "\n",
    "# Predict on the dev data\n",
    "preds = clf.predict(tree_dev_data)\n",
    "\n",
    "# And calculate the accuracy by comparing it to the labels\n",
    "correct, total = 0, 0\n",
    "for pred, label in zip(preds, tree_dev_labels):\n",
    "    if pred == label: \n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "# Finally we print the outcomes\n",
    "Outcome = [\"For the model with a minimum sample split of\", str(DT_optimal_sample_param), \"and a maximum depth of\",\n",
    "          str(DT_optimal_depth_param)]\n",
    "print(\" \".join(Outcome))\n",
    "print ('total: %3d  correct: %3d  accuracy: %3.3f' %(total, correct, 1.0*correct/total))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our best Decision Tree Classifier only achieves an accuracy in the high 60%s. This is not bad, but it fails to best our 1-Neighbor KNN model's score of 84%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.082815,
     "end_time": "2021-07-20T22:23:53.403623",
     "exception": false,
     "start_time": "2021-07-20T22:23:53.320808",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Random Forest is, in it's simplest explanation, just an ensemble (or group) of decision trees. First, it selects a random subset of the total training data, then it fits a decision tree to that subset of the data, creating a \"forest\" of multiple decision trees all trained on slightly different versions of the training data. When it comes time to make predictions, each new data point is run through all of the trees in the forest, with each tree getting a single vote as to it's class. The class with the highest votes is the prediction.\n",
    "\n",
    "There are three different important hyper parameters for us to tune in a Random Forest. Again we need to consider the minimum size sample needed to split and the max depth, but since we tuned that with our Decision Tree, and this is just an ensemble of those trees, we will keep them the same. The other parameter is the number of trees to include in the forest, \"n_estimators\".\n",
    "\n",
    "Because the possible sample space is too computationally intensive to search every possibility, we will use a new search method, RandomizedSearchCV, which searches a set number of random combinations and chooses the best possible parameters. We will look at 100 different possible parameters and search through 15% of them for the best possible option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T22:23:53.581126Z",
     "iopub.status.busy": "2021-07-20T22:23:53.580401Z",
     "iopub.status.idle": "2021-07-20T22:27:59.818728Z",
     "shell.execute_reply": "2021-07-20T22:27:59.819254Z",
     "shell.execute_reply.started": "2021-07-20T21:25:45.757967Z"
    },
    "papermill": {
     "duration": 246.331898,
     "end_time": "2021-07-20T22:27:59.819436",
     "exception": false,
     "start_time": "2021-07-20T22:23:53.487538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 15 candidates, totalling 30 fits\n",
      "[CV] n_estimators=213 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................. n_estimators=213, total=   4.0s\n",
      "[CV] n_estimators=213 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................................. n_estimators=213, total=   3.8s\n",
      "[CV] n_estimators=904 ................................................\n",
      "[CV] ................................. n_estimators=904, total=  16.7s\n",
      "[CV] n_estimators=904 ................................................\n",
      "[CV] ................................. n_estimators=904, total=  17.4s\n",
      "[CV] n_estimators=203 ................................................\n",
      "[CV] ................................. n_estimators=203, total=   3.5s\n",
      "[CV] n_estimators=203 ................................................\n",
      "[CV] ................................. n_estimators=203, total=   3.5s\n",
      "[CV] n_estimators=136 ................................................\n",
      "[CV] ................................. n_estimators=136, total=   2.6s\n",
      "[CV] n_estimators=136 ................................................\n",
      "[CV] ................................. n_estimators=136, total=   2.3s\n",
      "[CV] n_estimators=299 ................................................\n",
      "[CV] ................................. n_estimators=299, total=   5.4s\n",
      "[CV] n_estimators=299 ................................................\n",
      "[CV] ................................. n_estimators=299, total=   5.6s\n",
      "[CV] n_estimators=472 ................................................\n",
      "[CV] ................................. n_estimators=472, total=   8.8s\n",
      "[CV] n_estimators=472 ................................................\n",
      "[CV] ................................. n_estimators=472, total=  10.2s\n",
      "[CV] n_estimators=539 ................................................\n",
      "[CV] ................................. n_estimators=539, total=  10.0s\n",
      "[CV] n_estimators=539 ................................................\n",
      "[CV] ................................. n_estimators=539, total=  10.1s\n",
      "[CV] n_estimators=875 ................................................\n",
      "[CV] ................................. n_estimators=875, total=  19.7s\n",
      "[CV] n_estimators=875 ................................................\n",
      "[CV] ................................. n_estimators=875, total=  16.4s\n",
      "[CV] n_estimators=69 .................................................\n",
      "[CV] .................................. n_estimators=69, total=   1.7s\n",
      "[CV] n_estimators=69 .................................................\n",
      "[CV] .................................. n_estimators=69, total=   1.7s\n",
      "[CV] n_estimators=779 ................................................\n",
      "[CV] ................................. n_estimators=779, total=  14.7s\n",
      "[CV] n_estimators=779 ................................................\n",
      "[CV] ................................. n_estimators=779, total=  13.7s\n",
      "[CV] n_estimators=481 ................................................\n",
      "[CV] ................................. n_estimators=481, total=   9.0s\n",
      "[CV] n_estimators=481 ................................................\n",
      "[CV] ................................. n_estimators=481, total=   8.4s\n",
      "[CV] n_estimators=117 ................................................\n",
      "[CV] ................................. n_estimators=117, total=   2.0s\n",
      "[CV] n_estimators=117 ................................................\n",
      "[CV] ................................. n_estimators=117, total=   2.0s\n",
      "[CV] n_estimators=932 ................................................\n",
      "[CV] ................................. n_estimators=932, total=  17.8s\n",
      "[CV] n_estimators=932 ................................................\n",
      "[CV] ................................. n_estimators=932, total=  16.7s\n",
      "[CV] n_estimators=750 ................................................\n",
      "[CV] ................................. n_estimators=750, total=  14.0s\n",
      "[CV] n_estimators=750 ................................................\n",
      "[CV] ................................. n_estimators=750, total=  13.2s\n",
      "[CV] n_estimators=145 ................................................\n",
      "[CV] ................................. n_estimators=145, total=   2.7s\n",
      "[CV] n_estimators=145 ................................................\n",
      "[CV] ................................. n_estimators=145, total=   3.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:  4.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal value for n_estimators is 145\n",
      "For the model with 145 estimators \n",
      " a minimum sample split of 2 and a max depth of 27\n",
      "total: 1512  correct: 1052  accuracy: 0.696\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Now we search for the best\n",
    "paramsRF = {'n_estimators': [int(x) for x in np.linspace(start = 50, stop = 1000, num = 100)]}\n",
    "best_param_RF = RandomizedSearchCV(RandomForestClassifier(min_samples_split = DT_optimal_sample_param, \n",
    "                             max_depth = DT_optimal_depth_param), param_distributions = paramsRF, \n",
    "                             scoring='accuracy', n_iter = 15, cv = 2, verbose = 3)\n",
    "\n",
    "best_param_RF.fit(tree_train_data, tree_train_labels)\n",
    "RF_optimal_est_param = best_param_RF.best_params_['n_estimators']\n",
    "\n",
    "print ('The optimal value for n_estimators is {0}'.format(RF_optimal_est_param))\n",
    "            \n",
    "clf = RandomForestClassifier(n_estimators = RF_optimal_est_param, min_samples_split = DT_optimal_sample_param, \n",
    "                             max_depth = DT_optimal_depth_param, criterion='entropy')\n",
    "clf.fit(tree_train_data, tree_train_labels)\n",
    "# Predict on the dev data\n",
    "preds = clf.predict(tree_dev_data)\n",
    "\n",
    "# And calculate the accuracy by comparing it to the labels\n",
    "correct, total = 0, 0\n",
    "for pred, label in zip(preds, tree_dev_labels):\n",
    "    if pred == label: \n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "# Finally we print the outcomes\n",
    "Outcome = [\"For the model with\", str(RF_optimal_est_param), \"estimators\",\n",
    "          \"\\n a minimum sample split of\", str(DT_optimal_sample_param), \"and a max depth of\", str(DT_optimal_depth_param)]\n",
    "print(\" \".join(Outcome))\n",
    "print ('total: %3d  correct: %3d  accuracy: %3.3f' %(total, correct, 1.0*correct/total))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.088187,
     "end_time": "2021-07-20T22:27:59.996603",
     "exception": false,
     "start_time": "2021-07-20T22:27:59.908416",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Though we did manage to beat the results of our single decision tree, we still have not managed to do better than our baseline. Next we will attempt a slightly more complicated ensemble method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.088997,
     "end_time": "2021-07-20T22:28:00.175117",
     "exception": false,
     "start_time": "2021-07-20T22:28:00.086120",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost is an ensemble method similar to the Random Forest Algorithm. It fits a number of, in this case decision trees, which get to vote on predictions as before. The difference in this case is how the trees are fit. Adaboost attempts to train each new tree by teaching it to be better at the examples that the previous tree got wrong. It does this by fitting weights to each example. The weights for each example go up when a tree gets them wrong, and go down when a tree gets them right. The objective function for each new tree respects these weights, and puts more emphasis on fitting these difficult examples correctly.\n",
    "\n",
    "Adaboost with Decision Trees requires all the same parameters as our Random Forest, with one new one that must be optimized: the \"learning rate\". The learning rate determines how much the weights change between each iteration. We will fit Adaboost with all the optimal parameters determined before, but use Randomized Search CV to determine the optimal learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T22:28:00.361321Z",
     "iopub.status.busy": "2021-07-20T22:28:00.360183Z",
     "iopub.status.idle": "2021-07-20T22:30:41.780855Z",
     "shell.execute_reply": "2021-07-20T22:30:41.780239Z",
     "shell.execute_reply.started": "2021-07-20T16:17:23.645618Z"
    },
    "papermill": {
     "duration": 161.517904,
     "end_time": "2021-07-20T22:30:41.781125",
     "exception": false,
     "start_time": "2021-07-20T22:28:00.263221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 15 candidates, totalling 30 fits\n",
      "[CV] learning_rate=0.6067575757575757 ................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .... learning_rate=0.6067575757575757, score=0.605, total=  13.8s\n",
      "[CV] learning_rate=0.6067575757575757 ................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   13.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .... learning_rate=0.6067575757575757, score=0.639, total=  13.9s\n",
      "[CV] learning_rate=1.9596161616161614 ................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   27.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .... learning_rate=1.9596161616161614, score=0.576, total=  13.3s\n",
      "[CV] learning_rate=1.9596161616161614 ................................\n",
      "[CV] .... learning_rate=1.9596161616161614, score=0.542, total=   9.8s\n",
      "[CV] learning_rate=0.7884848484848485 ................................\n",
      "[CV] .... learning_rate=0.7884848484848485, score=0.575, total=   8.2s\n",
      "[CV] learning_rate=0.7884848484848485 ................................\n",
      "[CV] .... learning_rate=0.7884848484848485, score=0.599, total=   7.1s\n",
      "[CV] learning_rate=1.6365454545454543 ................................\n",
      "[CV] .... learning_rate=1.6365454545454543, score=0.563, total=   7.9s\n",
      "[CV] learning_rate=1.6365454545454543 ................................\n",
      "[CV] .... learning_rate=1.6365454545454543, score=0.547, total=   8.0s\n",
      "[CV] learning_rate=0.34426262626262627 ...............................\n",
      "[CV] ... learning_rate=0.34426262626262627, score=0.578, total=   9.2s\n",
      "[CV] learning_rate=0.34426262626262627 ...............................\n",
      "[CV] ... learning_rate=0.34426262626262627, score=0.622, total=   8.2s\n",
      "[CV] learning_rate=1.3336666666666666 ................................\n",
      "[CV] .... learning_rate=1.3336666666666666, score=0.593, total=   5.5s\n",
      "[CV] learning_rate=1.3336666666666666 ................................\n",
      "[CV] .... learning_rate=1.3336666666666666, score=0.598, total=   6.3s\n",
      "[CV] learning_rate=1.9798080808080807 ................................\n",
      "[CV] .... learning_rate=1.9798080808080807, score=0.516, total=   8.8s\n",
      "[CV] learning_rate=1.9798080808080807 ................................\n",
      "[CV] .... learning_rate=1.9798080808080807, score=0.500, total=   8.7s\n",
      "[CV] learning_rate=0.5461818181818182 ................................\n",
      "[CV] .... learning_rate=0.5461818181818182, score=0.586, total=   8.0s\n",
      "[CV] learning_rate=0.5461818181818182 ................................\n",
      "[CV] .... learning_rate=0.5461818181818182, score=0.558, total=   9.5s\n",
      "[CV] learning_rate=1.3134747474747472 ................................\n",
      "[CV] .... learning_rate=1.3134747474747472, score=0.605, total=  11.4s\n",
      "[CV] learning_rate=1.3134747474747472 ................................\n",
      "[CV] .... learning_rate=1.3134747474747472, score=0.538, total=   6.5s\n",
      "[CV] learning_rate=0.8086767676767677 ................................\n",
      "[CV] .... learning_rate=0.8086767676767677, score=0.590, total=   6.5s\n",
      "[CV] learning_rate=0.8086767676767677 ................................\n",
      "[CV] .... learning_rate=0.8086767676767677, score=0.589, total=   8.2s\n",
      "[CV] learning_rate=0.9904040404040404 ................................\n",
      "[CV] .... learning_rate=0.9904040404040404, score=0.639, total=   7.5s\n",
      "[CV] learning_rate=0.9904040404040404 ................................\n",
      "[CV] .... learning_rate=0.9904040404040404, score=0.655, total=   6.1s\n",
      "[CV] learning_rate=1.9192323232323232 ................................\n",
      "[CV] .... learning_rate=1.9192323232323232, score=0.539, total=   9.8s\n",
      "[CV] learning_rate=1.9192323232323232 ................................\n",
      "[CV] .... learning_rate=1.9192323232323232, score=0.535, total=   9.6s\n",
      "[CV] learning_rate=1.5961616161616161 ................................\n",
      "[CV] .... learning_rate=1.5961616161616161, score=0.555, total=   7.0s\n",
      "[CV] learning_rate=1.5961616161616161 ................................\n",
      "[CV] .... learning_rate=1.5961616161616161, score=0.574, total=   6.5s\n",
      "[CV] learning_rate=0.5057979797979798 ................................\n",
      "[CV] .... learning_rate=0.5057979797979798, score=0.592, total=  10.3s\n",
      "[CV] learning_rate=0.5057979797979798 ................................\n",
      "[CV] .... learning_rate=0.5057979797979798, score=0.567, total=   8.8s\n",
      "[CV] learning_rate=1.7576969696969695 ................................\n",
      "[CV] .... learning_rate=1.7576969696969695, score=0.565, total=   7.6s\n",
      "[CV] learning_rate=1.7576969696969695 ................................\n",
      "[CV] .... learning_rate=1.7576969696969695, score=0.556, total=   7.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:  4.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal value for learning rate is 0.9904040404040404\n",
      "For the model with 145 estimators \n",
      " a minimum sample split of 2 a max depth of 27 \n",
      " and a learning rate of 0.9904040404040404\n",
      "total: 1512  correct: 921  accuracy: 0.609\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Turning it into a form Adaboost likes\n",
    "tree_train_data = tree_train_data * 2 -1\n",
    "tree_dev_data = tree_dev_data * 2 -1\n",
    "\n",
    "#Now we search for the best learning rate\n",
    "paramsAD = {'learning_rate': [x for x in np.linspace(start = .001, stop = 2, num = 100)]}\n",
    "best_param_AD = RandomizedSearchCV(AdaBoostClassifier(base_estimator=DecisionTreeClassifier(\n",
    "    min_samples_split = DT_optimal_sample_param, max_depth = DT_optimal_depth_param), n_estimators=RF_optimal_est_param), \n",
    "                                   param_distributions = paramsAD, \n",
    "                             scoring='accuracy', n_iter = 15, cv = 2, verbose = 3)\n",
    "\n",
    "best_param_AD.fit(tree_train_data, tree_train_labels)\n",
    "AD_optimal_learn_param = best_param_AD.best_params_['learning_rate']\n",
    "\n",
    "print ('The optimal value for learning rate is {0}'.format(AD_optimal_learn_param))\n",
    "        \n",
    "\n",
    "#Now we fit our classifier           \n",
    "clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(min_samples_split = DT_optimal_sample_param, \n",
    "                                                               max_depth = DT_optimal_depth_param), \n",
    "                         n_estimators=RF_optimal_est_param, learning_rate=AD_optimal_learn_param)\n",
    "clf.fit(tree_train_data, tree_train_labels)\n",
    "# Predict on the dev data\n",
    "preds = clf.predict(tree_dev_data)\n",
    "\n",
    "# And calculate the accuracy by comparing it to the labels\n",
    "correct, total = 0, 0\n",
    "for pred, label in zip(preds, tree_dev_labels):\n",
    "    if pred == label: \n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "# Finally we print the outcomes\n",
    "Outcome = [\"For the model with\", str(RF_optimal_est_param), \"estimators\",\n",
    "          \"\\n a minimum sample split of\", str(DT_optimal_sample_param), \"a max depth of\", str(DT_optimal_depth_param),\n",
    "          \"\\n and a learning rate of\", str(AD_optimal_learn_param)]\n",
    "print(\" \".join(Outcome))\n",
    "print ('total: %3d  correct: %3d  accuracy: %3.3f' %(total, correct, 1.0*correct/total))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.102886,
     "end_time": "2021-07-20T22:30:41.991300",
     "exception": false,
     "start_time": "2021-07-20T22:30:41.888414",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model 3:\n",
    "## **Naive Bayes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "papermill": {
     "duration": 0.102422,
     "end_time": "2021-07-20T22:30:42.196385",
     "exception": false,
     "start_time": "2021-07-20T22:30:42.093963",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "First, we will be using a Bernoulli Naive Bayes. BernoulliNB is designed for binary/boolean features.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T22:30:42.407564Z",
     "iopub.status.busy": "2021-07-20T22:30:42.406222Z",
     "iopub.status.idle": "2021-07-20T22:30:42.411511Z",
     "shell.execute_reply": "2021-07-20T22:30:42.412030Z",
     "shell.execute_reply.started": "2021-07-20T16:18:57.980453Z"
    },
    "papermill": {
     "duration": 0.114243,
     "end_time": "2021-07-20T22:30:42.412217",
     "exception": false,
     "start_time": "2021-07-20T22:30:42.297974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#First we will create a copy of our data that we can modify\n",
    "NB_train_data = copy.deepcopy(train_data)\n",
    "NB_train_labels = copy.deepcopy(train_label)\n",
    "NB_dev_data = copy.deepcopy(dev_data)\n",
    "NB_dev_labels = copy.deepcopy(dev_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T22:30:42.622407Z",
     "iopub.status.busy": "2021-07-20T22:30:42.620854Z",
     "iopub.status.idle": "2021-07-20T22:30:42.687663Z",
     "shell.execute_reply": "2021-07-20T22:30:42.687071Z",
     "shell.execute_reply.started": "2021-07-20T16:18:58.011911Z"
    },
    "papermill": {
     "duration": 0.173847,
     "end_time": "2021-07-20T22:30:42.687806",
     "exception": false,
     "start_time": "2021-07-20T22:30:42.513959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Binarize our data\n",
    "binarize_data_for_tree(NB_train_data)\n",
    "binarize_data_for_tree(NB_dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T22:30:42.906685Z",
     "iopub.status.busy": "2021-07-20T22:30:42.905863Z",
     "iopub.status.idle": "2021-07-20T22:30:42.911684Z",
     "shell.execute_reply": "2021-07-20T22:30:42.910963Z",
     "shell.execute_reply.started": "2021-07-20T16:18:58.076372Z"
    },
    "papermill": {
     "duration": 0.121927,
     "end_time": "2021-07-20T22:30:42.911870",
     "exception": false,
     "start_time": "2021-07-20T22:30:42.789943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(NB_train_data.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T22:30:43.128980Z",
     "iopub.status.busy": "2021-07-20T22:30:43.128342Z",
     "iopub.status.idle": "2021-07-20T22:30:43.492133Z",
     "shell.execute_reply": "2021-07-20T22:30:43.491422Z",
     "shell.execute_reply.started": "2021-07-20T16:18:58.12122Z"
    },
    "papermill": {
     "duration": 0.476768,
     "end_time": "2021-07-20T22:30:43.492453",
     "exception": true,
     "start_time": "2021-07-20T22:30:43.015685",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = BernoulliNB(alpha = 1.0)\n",
    "clf.fit(NB_train_data, NB_train_labels)\n",
    "clf.predict(NB_train_data)\n",
    "clf.score(NB_dev_data,NB_dev_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Gaussian NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T16:18:58.173286Z",
     "iopub.status.busy": "2021-07-20T16:18:58.172905Z",
     "iopub.status.idle": "2021-07-20T16:18:58.197583Z",
     "shell.execute_reply": "2021-07-20T16:18:58.196687Z",
     "shell.execute_reply.started": "2021-07-20T16:18:58.173238Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NBG_train_data = copy.deepcopy(train_data)\n",
    "NBG_train_labels = copy.deepcopy(train_label)\n",
    "NBG_dev_data = copy.deepcopy(dev_data)\n",
    "NBG_dev_labels = copy.deepcopy(dev_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T16:18:58.228151Z",
     "iopub.status.busy": "2021-07-20T16:18:58.225255Z",
     "iopub.status.idle": "2021-07-20T16:18:58.259005Z",
     "shell.execute_reply": "2021-07-20T16:18:58.258193Z",
     "shell.execute_reply.started": "2021-07-20T16:18:58.228095Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(NBG_train_data, NBG_train_labels)\n",
    "clf.predict(NBG_train_data)\n",
    "clf.score(NBG_dev_data,NBG_dev_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Model 4:\n",
    "## **Logistic Regression**\n",
    "\n",
    "\n",
    "- One of the most common tools in supervised learning\n",
    "- A foundational tool in classification\n",
    "- A way to model the linear relationship between one or more arbitrary independent variables and binary dependent variables\n",
    "- Transforms the continuous infinite scale into a scale between 0 and 1.\n",
    "- uses maximum likelihood estimation instead of finding alpha or beta.\n",
    "- produces binary outcome variable in case of bonomial.\n",
    "- Related methods are:\n",
    "    - Multinomial logistic regression: when the outcome variable can take one of a set of categorical values whose size is greater than two\n",
    "    - Ordered logistic regression: used when the outcome variable is categorical with rank order (e.g., socioeconomic status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T18:52:44.104693Z",
     "iopub.status.busy": "2021-07-20T18:52:44.1043Z",
     "iopub.status.idle": "2021-07-20T18:52:56.127566Z",
     "shell.execute_reply": "2021-07-20T18:52:56.126473Z",
     "shell.execute_reply.started": "2021-07-20T18:52:44.104634Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using training data scaled to range [0,1]\n",
    "\n",
    "lr_pipe = Pipeline(steps = [\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('classifier', LogisticRegression(penalty = 'l2', solver='liblinear'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "lr_param_grid = {\n",
    "    'classifier__C': [1, 10, 100,1000],\n",
    "}\n",
    "\n",
    "\n",
    "#np.random.seed(1)\n",
    "grid_search = GridSearchCV(lr_pipe, lr_param_grid, cv=5, refit='True')\n",
    "grid_search.fit(train_data, train_label)\n",
    "\n",
    "print (\"### Using training data scaled to 0 -> 1 ###\\n\")\n",
    "print ('\\nOptimal C for Logistic Regression is {0}'.format(grid_search.best_params_))\n",
    "print ('Logistic Regression f1 Score is {0}'.format(grid_search.best_score_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Model 5:\n",
    "## **Stochastic Gradient Descent (SGD)**\n",
    "\n",
    "Gradient descent is an iterative algorithm, that starts from a random point on a function and travels down its slope in steps until it reaches the lowest point of that function.\n",
    "\n",
    "Stochastic Gradient Descent(SGD) helps in inducing randomness in the gradient descent algorithm and hence decreasing the computation overhead.\n",
    "\n",
    "SGD is induced while selecting data points at each step to calculate the derivatives. \n",
    "SGD randomly picks one data point from the whole data set at each iteration to reduce the computations enormously.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T18:59:50.343539Z",
     "iopub.status.busy": "2021-07-20T18:59:50.343163Z",
     "iopub.status.idle": "2021-07-20T18:59:50.540624Z",
     "shell.execute_reply": "2021-07-20T18:59:50.538798Z",
     "shell.execute_reply.started": "2021-07-20T18:59:50.343508Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "sgd_clf = linear_model.SGDClassifier(alpha=0.001)\n",
    "sgd_clf.fit(train_data_minmax, train_label)\n",
    "pred_sgd= sgd_clf.predict(dev_data_minmax)\n",
    "#sgd_clf.fit(train_data_b, train_label)\n",
    "#pred_sgd= sgd_clf.predict(dev_data_b)\n",
    "print (metrics.classification_report(dev_label, pred_sgd))\n",
    "print (\" Accuracy score\", metrics.accuracy_score(dev_label, pred_sgd))\n",
    "print(\"\\nConfusion metric\", metrics.confusion_matrix(dev_label, pred_sgd))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "***Accuracy score with SGD is 0.64***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Model 6:\n",
    "## **Support Vector Machine**\n",
    "\n",
    "The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N  the number of features) that distinctly classifies the data points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T18:53:40.481569Z",
     "iopub.status.busy": "2021-07-20T18:53:40.481186Z",
     "iopub.status.idle": "2021-07-20T18:59:03.012975Z",
     "shell.execute_reply": "2021-07-20T18:59:03.011934Z",
     "shell.execute_reply.started": "2021-07-20T18:53:40.481513Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# finding the best parameters\n",
    "param_grid = {'C': [1, 10, 100, 1000], \n",
    "              'gamma': [1, 0.5, 0.1, 0.01], \n",
    "              'kernel': ['rbf']}\n",
    "best_svm = GridSearchCV(SVC(), param_grid, scoring='accuracy')\n",
    "best_svm.fit(train_data_minmax, train_label)\n",
    "best_svm.score(dev_data_minmax, dev_label)\n",
    "print (best_svm.best_params_)\n",
    "\n",
    "svm = SVC(kernel = best_svm.best_params_['kernel'], C=best_svm.best_params_['C'],\n",
    "          gamma=best_svm.best_params_['gamma'])\n",
    "svm.fit(train_data_minmax, train_label)\n",
    "svm_preds = svm.predict(dev_data_minmax)\n",
    "print (metrics.accuracy_score(dev_label, svm_preds))\n",
    "print (classification_report(dev_label, svm_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Accuracy score with SVM is 0.85***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running SVM with the best parameter identified above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-20T18:59:32.425418Z",
     "iopub.status.busy": "2021-07-20T18:59:32.425052Z",
     "iopub.status.idle": "2021-07-20T18:59:40.630586Z",
     "shell.execute_reply": "2021-07-20T18:59:40.628642Z",
     "shell.execute_reply.started": "2021-07-20T18:59:32.425386Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Running SVM with the best parameter identified above\n",
    "SVM = SVC(kernel='rbf', C=1000, gamma=0.5)\n",
    "SVM.fit(train_data_minmax, train_label)\n",
    "pred_y_dev_SVM = SVM.predict(dev_data_minmax)\n",
    "acc_SVM = metrics.accuracy_score(dev_label, pred_y_dev_SVM)\n",
    "print (acc_SVM)\n",
    "CM = (metrics.confusion_matrix(dev_label, pred_y_dev_SVM))\n",
    "print (CM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 7:\n",
    "\n",
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking ahead to working with neural networks, let's prepare one additional variation of the label data. Let's make these labels, rather than each being an integer value from 1-7, be a set of 7 binary values, one for each class. This is sometimes called a 1-of-n encoding, and it makes working with Neural Networks easier, as there will be one output node for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarizeY(data):\n",
    "    binarized_data = np.zeros((data.size, 8))\n",
    "    for j in range(0, data.size):\n",
    "        feature = data[j:j+1]\n",
    "        i = feature.astype(np.int64) \n",
    "        binarized_data[j, i] = 1\n",
    "    return binarized_data\n",
    "train_label_b = binarizeY(train_label)\n",
    "dev_label_b = binarizeY(dev_label)\n",
    "numClasses = train_label_b[1].size\n",
    "print(train_label_b.shape)\n",
    "print ('Classes = %d' %(numClasses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model\n",
    "import time\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "\n",
    "numTrainExamples = train_data.shape[0]\n",
    "\n",
    "## Model\n",
    "model = Sequential() \n",
    "model.add(Dense(8, input_dim=54, activation='softmax')) \n",
    "\n",
    "## Cost function & Objective (and solver)\n",
    "sgd = optimizers.SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "start_time = time.time()\n",
    "history = model.fit(train_data, train_label_b, shuffle=False, batch_size=numTrainExamples, verbose=0, epochs=50) \n",
    "print ('Train time = %.2f' %(time.time() - start_time))\n",
    "score = model.evaluate(dev_data, dev_label_b, verbose=0) \n",
    "#score = model.evaluate(dev_data, dev_label, verbose=0) \n",
    "print('Dev score:', score[0]) \n",
    "print('Dev accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-layer Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take our implementation of single layer neural network (which recall is in fact a logistic regression), and add a hidden layer, making it a two layer neural network. Because we have a hidden layer, we will now train the model using backpropagation.\n",
    "\n",
    "Let's try how this model performs as compared to KNN and logistic regression in terms of train time and accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model\n",
    "model = Sequential() \n",
    "model.add(Dense(units=28, input_dim=54, activation='sigmoid')) \n",
    "model.add(Dense(units=8, input_dim=28, activation='softmax')) \n",
    "\n",
    "## Cost function & Objective (and solver)\n",
    "sgd = optimizers.SGD(learning_rate=0.01)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(train_data, train_label_b, shuffle=False, batch_size=10,verbose=0, epochs=50) \n",
    "score = model.evaluate(dev_data, dev_label_b, verbose=0) \n",
    "print('Dev score:', score[0]) \n",
    "print('Dev accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model\n",
    "model = Sequential() \n",
    "model.add(Dense(units=28, input_dim=54, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=8, input_dim=28, activation='softmax')) \n",
    "\n",
    "## Cost function & Objective (and solver)\n",
    "sgd = optimizers.SGD(learning_rate=0.01)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(train_data, train_label_b, shuffle=False, batch_size=5,verbose=0, epochs=50) \n",
    "score = model.evaluate(dev_data, dev_label_b, verbose=0) \n",
    "print('Dev score:', score[0]) \n",
    "print('Dev accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised learning approach - GMM with PCA\n",
    "We also tried an unsupervised learning approach, where we loop through a combination of Gaussian Mixture models and dimensionality reduction via Principal Component Analysis in order to try and predict labels. \n",
    "\n",
    "Utilizing this approach yields a dev accuracy of just 14% in the best scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Including component weights\n",
    "# full = ((n_gmm - 1) +n_pca*n_gmm + n_pca (n_pca + 1)/ 2 * n_gmm) * n_classes\n",
    "# diagonal = ((n_gmm - 1) +n_gmm + n_pca*n_gmm + n_pca * n_gmm) * n_classes\n",
    "# spherical = ((n_gmm - 1) +n_pca*n_gmm + n_gmm) * n_classes\n",
    "# tied = ((n_gmm - 1) +n_pca*n_gmm + n_pca (n_pca + 1)/ 2) * n_classes\n",
    "\n",
    "def gmm_accuracy(pca_comps=2, gmm_components=4, covar_type='full'):\n",
    "    pca = PCA(n_components=pca_comps)\n",
    "    pca.fit(train_data)\n",
    "    pca_data = pca.transform(train_data)\n",
    "    pca_dev_data = pca.transform(dev_data)\n",
    "             \n",
    "    gmm_model = GaussianMixture(n_components = gmm_components, covariance_type=covar_type, random_state=12345)\n",
    "        \n",
    "    predictions = np.argmax(gmm_model, axis=0) + 1\n",
    "\n",
    "    #Calculate accuracy\n",
    "    accuracy=np.sum(predictions == dev_label) / len(dev_data)\n",
    "\n",
    "    return accuracy\n",
    "    \n",
    "def num_parameters(pca_comps=2, gmm_comps=4, covar_type='full', num_class=2):\n",
    "        \n",
    "    mean_vectors = pca_comps\n",
    "    mean_params = pca_comps * gmm_comps\n",
    "    comp_weights = gmm_comps - 1\n",
    "        \n",
    "# covariance matrix from spreadsheet\n",
    "    if covar_type == \"full\" or covar_type == \"diag\" or covar_type == \"spherical\":\n",
    "        if covar_type == \"full\" :\n",
    "            covar_mtx = (pca_comps * (pca_comps + 1)) / 2\n",
    "            cov_params = gmm_comps * covar_mtx\n",
    "                \n",
    "        elif covar_type == \"diag\":\n",
    "            covar_mtx = pca_comps\n",
    "            cov_params = gmm_comps * covar_mtx\n",
    "                \n",
    "        elif covar_type == \"spherical\":\n",
    "            covar_mtx = 1\n",
    "            cov_params = gmm_comps * covar_mtx\n",
    "            \n",
    "    elif covar_type == \"tied\":\n",
    "        covar_mtx = pca_comps * (pca_comps + 1) / 2\n",
    "        cov_params = covar_mtx\n",
    "                 \n",
    "    parameters = int((mean_params + cov_params + comp_weights) * num_class)\n",
    "    return parameters\n",
    "\n",
    "columns = ['PCA Components', 'GMM Components', 'Covariance Type', \"Num of Parameters\", \"Accuracy [%]\"]\n",
    "gmm_trials = []\n",
    "    \n",
    "num_class = 2\n",
    "max_pca_comps = 25\n",
    "max_gmm_comps = 25\n",
    "covar_types = ['full', 'diag', 'spherical', 'tied']\n",
    "    \n",
    "for pca_comps in range(1, max_pca_comps):\n",
    "    for gmm_comps in range(1, max_gmm_comps):\n",
    "        for covar_type in covar_types:\n",
    "            parameters = num_parameters(pca_comps, gmm_comps, covar_type, num_class)\n",
    "            if parameters <= 50:\n",
    "                accuracy = gmm_accuracy(pca_comps, gmm_comps, covar_type)\n",
    "                gmm_trials.append([pca_comps, gmm_comps, covar_type, parameters, round(accuracy*100, 2)])\n",
    "                    \n",
    "\n",
    "#GMM Trial results, sort, and construct for table plot\n",
    "gmm_trials = np.array(gmm_trials)\n",
    "gmm_trials = gmm_trials[gmm_trials[:,4].argsort()]\n",
    "    \n",
    "concat_var = (f'The best accuracy of {gmm_trials[-1,4]}% was obtained using'\n",
    "                  f' {gmm_trials[-1,1]} GMM component with a {gmm_trials[-1,2]} covariance type,'\n",
    "                  f' and {gmm_trials[-1,0]} PCA components')\n",
    "print(concat_var)\n",
    "\n",
    "print(f'Table with experiment results')\n",
    "    \n",
    "#Print and plot accuracies of the experiments\n",
    "plt.figure(figsize=(15, 15))\n",
    "ax = plt.gca()\n",
    "ax.axis('off')\n",
    "plt.table(cellText=gmm_trials, colLabels=columns, loc=\"top\")\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 549.117555,
   "end_time": "2021-07-20T22:30:45.872509",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-07-20T22:21:36.754954",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
